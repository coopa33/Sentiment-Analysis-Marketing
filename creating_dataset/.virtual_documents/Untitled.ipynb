





from nltk.tokenize import TreebankWordTokenizer
from nltk.corpus import wordnet as wn
from nltk import sent_tokenize, pos_tag
from nltk.stem import WordNetLemmatizer
from nltk.corpus import sentiwordnet as swn
import nltk
import pandas as pd
import numpy as np
import seaborn as sns
nltk.download('averaged_perceptron_tagger_eng')
nltk.download("sentiwordnet")


df_small = pd.read_csv("../data/small_corpus.csv")



df_small.head()


# Function to convert PennTreebank tags to Wordnet tags
nltk.download("wordnet")

def penn_to_wn(tag):
    """
    Convert PennTreebank tags to simpler Wordnet tags
    """
    if tag.startswith("J"): # Adjectives start with J in PennTreebank tags
        return wn.ADJ
    elif tag.startswith("N"):
        return wn.NOUN
    elif tag.startswith("R"):
        return wn.ADV
    elif tag.startswith("V"):
        return wn.VERB

    return None


penn_to_wn("JJR") # JJR = Adjective, comparative. In PennTreebank POS


from nltk import sent_tokenize, pos_tag


# Test sentence and word tokenizer
review_tokens = df_small["reviewText"].apply(str)
review_tokens = review_tokens.apply(sent_tokenize)
sentence_tokens = review_tokens.loc[0]
sentence_token = sentence_tokens[1]
sentence_token


word_tokens = TreebankWordTokenizer().tokenize(sentence_token)
word_tokens


tags = pos_tag(word_tokens)
print(tags)


for word, tag in tags:
    tag = penn_to_wn(tag)
    if not tag:
        continue
    lemma = WordNetLemmatizer().lemmatize(word, pos=tag)
    print(lemma)
    


            
            

def lemmatize_corpus(text):
    lemmas = []
    sentence_tokens = sent_tokenize(text)
    #print(sentence_tokens)
    for sentence_token in sentence_tokens:
        # Tokenize sentence into words
        word_tokens = TreebankWordTokenizer().tokenize(sentence_token)

        # Make POS tag tuples
        wn_tags = pos_tag(word_tokens)
        
        for token, tag in wn_tags:
            token = token.lower()
            # Convert any applicable PennTreebank tags to Wordnet tags. If not applicable
            # for lemmatization (not noun, verb, adjective or adverb), returns None, and therefore
            # don't use word 
            wn_tag = penn_to_wn(tag)
            if not wn_tag:
                continue
            lemma = WordNetLemmatizer().lemmatize(token, wn_tag)
            lemmas.append((lemma, wn_tag))

    return lemmas
    
            
        
        


text = "She had a little lamb. The lamb was very young and vulnerable!"
lemmatize_corpus(text)


sample = df_small.head().copy(deep=True)
sample["lemmatized"] = sample["reviewText"].apply(lemmatize_corpus)
sample["tags"] = sample["reviewText"].apply(tokenize)



print(sample["reviewText"][1])
print(sample["lemmatized"][1])


sample.head()


# Test out SentiwordNet
lemma = sample["lemmatized"][1][1]
synset = wn.synsets(lemma[0], pos=lemma[1])


display(synset)





synset = synset[0]


display(synset)

# Name of synset object can be accessed by method
display(synset.name())


swn_synset = swn.senti_synset(synset.name())


display(swn_synset)





display(swn_synset.pos_score())
display(swn_synset.neg_score())





senti_score = swn_synset.pos_score() - swn_synset.neg_score()
senti_score





def sentiment_score(text):
    sentence_tokens = sent_tokenize(text)
    total_score = 0
    for sentence_token in sentence_tokens:
        sentence_score = 0
        no_words_used = 0
        word_tokens = TreebankWordTokenizer().tokenize(sentence_token)
        pos_tuple = pos_tag(word_tokens)
        for word, tag in pos_tuple:
            wn_tag = penn_to_wn(tag)
            if not wn_tag:
                continue
            lemma = WordNetLemmatizer().lemmatize(word, pos=wn_tag)
            if not lemma:
                continue
            synset = wn.synsets(lemma, pos=wn_tag)
            if not synset:
                continue
            synset = synset[0] # Select most common synset

            # Transform into SentiSynset
            swn_synset = swn.senti_synset(synset.name())
            sentence_score += swn_synset.pos_score() - swn_synset.neg_score()

            # Word was used, add count
            no_words_used += 1

        # Average sentence score across words in the sentence, and add to total
        if no_words_used == 0:
            continue
        total_score += (sentence_score/no_words_used)
        

    return total_score


sample = df_small.head(100).copy(deep=True)
sample.head()


sample["sentiment_score"] = sample["reviewText"].apply(sentiment_score)


sample.head()



